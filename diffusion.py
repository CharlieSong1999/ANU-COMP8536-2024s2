from PIL import Image, ImageDraw
import torch
import matplotlib.pyplot as plt
import numpy as np
from torchvision.transforms.functional import resize
from diffusers import StableDiffusionUpscalePipeline
from diffusers.utils import load_image
import json
import os


def create_mask(image, mask_type="rectangle", region=None, invert=False):
    """
    Create a mask based on the specified mask type and region.

    Args:
    image: PIL.Image
        Original image on which to apply the mask.
    mask_type: str, optional
        Type of the mask ('rectangle', 'circle', etc.).
    region: tuple, optional
        Coordinates for the mask region as a percentage (left_pct, upper_pct, right_pct, lower_pct).
    invert: bool, optional
        If True, inverts the mask so the area outside the region is masked instead of the inside.

    Returns:
    mask: PIL.Image
        Generated mask image.
    """
    mask = Image.new("L", image.size, 255 if invert else 0)  # Create a white mask if inverted, else black
    draw = ImageDraw.Draw(mask)
    width, height = image.size

    if mask_type == "rectangle" and region is not None:
        # Convert percentage-based region to pixel values
        left = int(region[0] * width)
        upper = int(region[1] * height)
        right = int(region[2] * width)
        lower = int(region[3] * height)
        if invert:
            draw.rectangle([left, upper, right, lower], fill=0)  # Black rectangle in the region for inverted mask
        else:
            draw.rectangle([left, upper, right, lower], fill=255)  # White rectangle for the masked area
    elif mask_type == "circle" and region is not None:
        center_x = int(region[0] * width)
        center_y = int(region[1] * height)
        radius = int(region[2] * min(width, height))  # Scale radius to the smaller dimension
        if invert:
            draw.ellipse((center_x - radius, center_y - radius, center_x + radius, center_y + radius),
                         fill=0)  # Black circle for inverted mask
        else:
            draw.ellipse((center_x - radius, center_y - radius, center_x + radius, center_y + radius),
                         fill=255)  # White circle for masked area
    else:
        raise ValueError(f"Invalid mask type or region: {mask_type}, {region}")

    return mask


def upscale_image(image, target_size, prompt="A high-quality image"):
    # 加载预训练的Stable Diffusion上采样管道
    pipeline = StableDiffusionUpscalePipeline.from_pretrained(
        "stabilityai/stable-diffusion-x4-upscaler",
        torch_dtype=torch.float16
    ).to("cuda")

    # 确保输入图像是 RGB 格式
    if image.mode != "RGB":
        image = image.convert("RGB")

    # 调整输入图片的大小以适配模型
    original_size = image.size  # 保存原始尺寸

    # 使用Stable Diffusion上采样进行上采样
    upscaled_image = pipeline(prompt=prompt, image=image).images[0]

    # 将上采样后的图像调整为目标尺寸
    upscaled_image = upscaled_image.resize(target_size, Image.Resampling.LANCZOS)

    # Clean up
    del pipeline
    torch.cuda.empty_cache()

    return upscaled_image


def overlay_mask_on_image(image, mask, color=(255, 0, 0), alpha=0.5):
    """
    Overlay the mask on the original image with a given color and transparency.

    Args:
    image: PIL.Image
        Original image.
    mask: PIL.Image
        Binary mask image.
    color: tuple
        RGB color for the mask overlay (default is red).
    alpha: float
        Transparency level of the mask overlay (0 = fully transparent, 1 = fully opaque).

    Returns:
    PIL.Image: Image with mask overlay.
    """
    image = image.convert("RGBA")
    mask = mask.convert("L")

    overlay = Image.new("RGBA", image.size, color + (0,))
    mask_color = Image.new("RGBA", image.size, color + (int(alpha * 255),))
    overlay = Image.composite(mask_color, overlay, mask)
    blended_image = Image.alpha_composite(image, overlay)

    return blended_image.convert("RGB")


def visualize_inpainting_comparison(image, mask, inpainted_image):
    """
    Visualize the original image, mask overlay on the original image, and the inpainted result.

    Args:
    image: PIL.Image
        Original image resized to 1024x1024.
    mask: PIL.Image
        Mask image resized to 1024x1024.
    inpainted_image: PIL.Image
        The inpainted image generated by the pipeline (1024x1024).
    """
    image_with_mask = overlay_mask_on_image(image, mask)

    fig, axs = plt.subplots(1, 3, figsize=(15, 5))

    axs[0].imshow(image)
    axs[0].set_title("Original Image (Resized)")
    axs[0].axis("off")

    axs[1].imshow(image_with_mask)
    axs[1].set_title("Image with Mask")
    axs[1].axis("off")

    axs[2].imshow(inpainted_image)
    axs[2].set_title("Inpainted Image")
    axs[2].axis("off")

    plt.savefig('./outputs/compare_inpaint.jpg')
    plt.show()


def get_Inpainting_Pipeline(base_model='stable-diffusion'):
    """
    Get the inpainting pipeline from the Hugging Face model hub.

    Args:
    base_model: str, default 'stable-diffusion'
        The base model to use for inpainting. Options are 'stable-diffusion' and 'kandinsky-2-2'.

    Returns:
    pipeline: Inpainting pipeline
    """
    if base_model == 'stable-diffusion':
        from diffusers import StableDiffusionInpaintPipeline
        pipeline = StableDiffusionInpaintPipeline.from_pretrained(
            "stabilityai/stable-diffusion-2-inpainting",
            torch_dtype=torch.float16,
        )
        pipeline.to("cuda")
    elif base_model == 'kandinsky-2-2':
        from diffusers import AutoPipelineForInpainting
        pipeline = AutoPipelineForInpainting.from_pretrained(
            "kandinsky-community/kandinsky-2-2-decoder-inpaint", torch_dtype=torch.float16
        )
        pipeline.to("cuda")
    elif base_model == 'stable-diffusion-xl':
        from diffusers import AutoPipelineForInpainting
        pipeline = AutoPipelineForInpainting.from_pretrained(
            "diffusers/stable-diffusion-xl-1.0-inpainting-0.1", torch_dtype=torch.float16, variant="fp16"
        )
        pipeline.to("cuda")
    else:
        raise ValueError(f"Invalid base_model: {base_model}")

    pipeline.enable_model_cpu_offload()
    pipeline.enable_xformers_memory_efficient_attention()

    return pipeline


def run_inpainting(image_path, mask_type="rectangle", mask_region=None, base_model='stable-diffusion', seed=1024,
                   prompt_diffusion=None, strength=0.7, negative_prompt=None):
    """
    Run the inpainting process with a selectable mask type and visualize the results.

    Args:
    image_path: str
        Path to the original image.
    mask_type: str
        Type of mask to use ('rectangle', 'circle').
    mask_region: tuple
        Coordinates or region for the mask as percentages (e.g., (0.1, 0.1, 0.8, 0.8) for rectangle).
    base_model: str
        The inpainting model to use.
    seed: int
        Random seed for reproducibility.
    prompt_diffusion: str
        Custom diffusion prompt.
    strength: float
        Strength of the inpainting process.
    negative_prompt: str
        Optional negative prompt to steer the diffusion.

    Returns:
    inpainted_image: PIL.Image
        The resulting inpainted image.
    """
    # Load the original image
    image = load_image(image_path)
    original_size = image.size  # Keep the original size

    # Resize image for inpainting process
    # image_resized = resize(image, [1024, 1024])
    image_resized = image

    # Generate or load a mask based on the type and region (mask region is in percentages)
    mask = create_mask(image_resized, mask_type, region=mask_region, invert=True)

    image_with_mask = overlay_mask_on_image(image_resized, mask)
    image_with_mask.save(f'./outputs/mask_{os.path.basename(image_path)}')

    # Load the pipeline
    pipeline = get_Inpainting_Pipeline(base_model)

    # Inpaint the image
    generator = torch.Generator("cuda").manual_seed(seed)
    prompt = prompt_diffusion if prompt_diffusion else "A beautiful landscape with a sunset."
    inpainted_image = \
        pipeline(prompt=prompt, image=image_resized, mask_image=mask, generator=generator, strength=strength,
                 negative_prompt=negative_prompt, num_inference_steps=100).images[0]

    # Resize the inpainted image back to the original size
    inpainted_image = inpainted_image.resize(original_size)

    # Visualize and save the results (all at 1024x1024 resolution)
    visualize_inpainting_comparison(image_resized, mask, inpainted_image)

    # Save the inpainted image at original size
    inpainted_image.save(f'./outputs/inpainting_{os.path.basename(image_path)}')

    # Clean up
    del pipeline
    torch.cuda.empty_cache()

    return inpainted_image


if __name__ == '__main__':
    # # inpainting
    # image_path = './dataset/selected/8a_gameroom2.jpg' #6.jpg' #
    #
    # json_file_path = os.path.dirname(image_path) + '/image_prompt_mapping.json'
    # with open(json_file_path, 'r') as f:
    #     img_prompt_mapping = json.load(f)
    # prompt = img_prompt_mapping[os.path.basename(image_path)]
    # print(prompt)
    #
    # # mask_region specified in percentages of image dimensions (left_pct, upper_pct, right_pct, lower_pct)
    # mask_region = (0, 0, 0.8, 0.9)
    #
    # inpainted_img = run_inpainting(image_path, mask_type="rectangle", mask_region=mask_region,
    #                                prompt_diffusion=prompt)


    image = load_image('./image.png')
    mask = load_image('./mask.png')

    image = resize(image, [512, 512])
    mask = resize(mask, [512, 512])

    base_model ='stable-diffusion'
    pipeline = get_Inpainting_Pipeline(base_model)
    seed = 1024
    strength = 0.6
    generator = torch.Generator("cuda").manual_seed(seed)
    prompt = "The image shows a room with a blue pool table in the center. The walls are painted a light gray color, and there is a large screen TV mounted on one wall. The room has a modern look, with a minimalist aesthetic. The floor is made of hardwood, and there are a few pieces of furniture, including a small table and a couple of chairs."

    inpainted_image = \
        pipeline(prompt=prompt, image=image, mask_image=mask, generator=generator, strength=strength,
                 negative_prompt=None, num_inference_steps=100).images[0]
    visualize_inpainting_comparison(image, mask, inpainted_image)

    #upsampling
    # image_path = './outputs/inpainting_6.jpg'
    # image = load_image(image_path)
    # original_size = image.size  # Keep the original size
    # upscaled_image = upscale_image(image, target_size=original_size)
    # upscaled_image.save(f'./outputs/upsampling_{os.path.basename(image_path)}')
